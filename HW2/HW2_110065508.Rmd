---
title: 'HW2: Classification'
author: "Cheng-En Lee, 110065508"
date: "due on 10/25 (Tue) 9am"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

### Data Source

```{r}
library(mlbench) #install package first!!
library(corrplot)
library(MASS)
library(Hmisc)
library(class)
library(nnet)
library(glmnet)
```

## Problem 1: Wisconsin Breast Cancer Data

### 1. EDA

(1) Data preprocessing:

These data consist of 699 observations on 11 variables, one being "ID" variable, 9 being ordered or nominal variables, and 1 target class 

```{r, fig.height = 9, fig.width = 9}
data(BreastCancer)
head(BreastCancer)
dim(BreastCancer)

#make variables numeric (remove variable:ID) and save the data as a dataframe object
dat1 = matrix(as.numeric(as.matrix(BreastCancer[,2:10])), 699, 9) 
dat1 = data.frame(dat1)
colnames(dat1) <- colnames(BreastCancer)[2:10]
head(dat1)
dat1$case = as.numeric(BreastCancer$Class=="malignant")
```

(2) There are 16 NA's in variable **Bare.nuclei**. Hence, I only use the observations with complete data.

```{r, fig.height = 9, fig.width = 9}
#remove missing data (NA)
dat1 = na.omit(dat1)
dim(dat1) #check data dimension
```

(3) Plot the scatter plot and the histrogram of the dataset

```{r, fig.height = 9, fig.width = 9}
pairs(dat1[,1:9], col=as.factor(dat1[,10]), pch="+")
```

These features has a integer value. Besides, some of the features \texttt{Cl.thickness}, \texttt{Cell.size}, \texttt{Cell.shape}, and \texttt{Marg.adhesion} seems to separate the two classes. 

```{r, fig.height = 9, fig.width = 9}
hist.data.frame(dat1[,1:9])
```

This shows that the features does not follows a Gaussian distribution. Some of the assumptions using in data may need to be adjust.

(3) Correlation plots

```{r, fig.height = 6, fig.width = 6}
#view variable correlations:
round(cor(dat1),2)
corrplot(cor(dat1))
```

There are high correlations between all input variables.

(4) There is a class unbalance, but not severe.

```{r, fig.height = 6, fig.width = 6}
as.data.frame(table(dat1$case))
```

### 2. Performing the classification task

Do the train test split first. The splitting proportion is set to 0.7.

```{r, fig.height = 6, fig.width = 6}
set.seed(48763)
sample <- sample(c(TRUE, FALSE), nrow(dat1), replace=TRUE, prob=c(0.7,0.3))
train <- dat1[sample, ]
test <- dat1[!sample, ]
```

(1) logistic regression

Let's consider the vanilla logistic regression with all features.

```{r, fig.height = 6, fig.width = 6}
glm.fits <- glm(
    case ~  Cl.thickness + Cell.size + Cell.shape + Marg.adhesion + 
            Epith.c.size + Bare.nuclei + Bl.cromatin + Normal.nucleoli + 
            Mitoses,
    data = train, 
    family = binomial
  )
summary(glm.fits)
```

Making predictions on both the training set and the testing set, and derive the confusion matrix. The performance looks good since overall training accuracy is 97.93% and the testing accuracy is 0.93%.

```{r, fig.height = 6, fig.width = 6}
# Predicting on the training set
glm.probs <- predict(glm.fits, train, type = "response")
glm.pred <- rep(0, length(train$case))
glm.pred[glm.probs > .5] = 1
table(glm.pred, train$case)
mean(glm.pred == train$case)

# Predicting on the test set
glm.probs_test <- predict(glm.fits, test, type = "response")
glm.pred_test <- rep(0, length(test$case))
glm.pred_test[glm.probs_test > .5] = 1
table(glm.pred_test, test$case)
mean(glm.pred_test == test$case)
```

Let's use backward selection [1] to choose important features to see if further improvement can be performed.

```{r, fig.height = 6, fig.width = 6}
glm.fits <- glm(
    case ~  Cl.thickness + Cell.size + Marg.adhesion + 
            Epith.c.size + Bare.nuclei + Bl.cromatin + Normal.nucleoli + 
            Mitoses,
    data = train, 
    family = binomial
  )
summary(glm.fits)
```

```{r, fig.height = 6, fig.width = 6}
glm.fits <- glm(
    case ~  Cl.thickness + Marg.adhesion + 
            Epith.c.size + Bare.nuclei + Bl.cromatin + Normal.nucleoli + 
            Mitoses,
    data = train, 
    family = binomial
  )
summary(glm.fits)
```

```{r, fig.height = 6, fig.width = 6}
glm.fits <- glm(
    case ~  Cl.thickness + Marg.adhesion + 
            Bare.nuclei + Bl.cromatin + Normal.nucleoli + 
            Mitoses,
    data = train, 
    family = binomial
  )
summary(glm.fits)
```

```{r, fig.height = 6, fig.width = 6}
glm.fits <- glm(
    case ~  Cl.thickness + Marg.adhesion + 
            Bare.nuclei + Bl.cromatin + Normal.nucleoli,
    data = train, 
    family = binomial
  )
summary(glm.fits)
```

Making predictions again.

```{r, fig.height = 6, fig.width = 6}
# Predicting on the training set
glm.probs <- predict(glm.fits, train, type = "response")
glm.pred <- rep(0, length(train$case))
glm.pred[glm.probs > .5] = 1
table(glm.pred, train$case)
mean(glm.pred == train$case)

# Predicting on the test set
glm.probs_test <- predict(glm.fits, test, type = "response")
glm.pred_test <- rep(0, length(test$case))
glm.pred_test[glm.probs_test > .5] = 1
table(glm.pred_test, test$case)
mean(glm.pred_test == test$case)
```

The performance improved to 98.14% and 95%, respectively. Also, the gap between training and testing is reduced. This is because through the backward selection, the noise and the non-important features are filtered out, and the complexity of model has thus reduced. 

(2) Linear Discriminate Analysis

```{r, fig.height = 6, fig.width = 6}
lda.fit <- lda(case ~ Cl.thickness + Cell.size + Cell.shape + 
                      Marg.adhesion + Epith.c.size + Bare.nuclei + 
                      Bl.cromatin + Normal.nucleoli + Mitoses, 
               data = train)
lda.fit
plot(lda.fit)
```

Let's see the prediction results.

```{r, fig.height = 6, fig.width = 6}
# Predict on training set
lda.pred <- predict(lda.fit, train)
lda.class <- lda.pred$class
table(lda.class, train$case)
mean(lda.class == train$case)

# Predict on testing set
lda.pred_test <- predict(lda.fit, test)
lda.class <- lda.pred_test$class
table(lda.class, test$case)
mean(lda.class == test$case)
```

Since the LDA and logistic regression are almost the same given the same features, let's consider the LDA with features selected in (1).

```{r, fig.height = 6, fig.width = 6}
lda2.fit <- lda(case ~ Cl.thickness + Marg.adhesion + Bare.nuclei + 
                      Bl.cromatin + Normal.nucleoli, 
               data = train)
lda2.fit
plot(lda2.fit)
```

```{r, fig.height = 6, fig.width = 6}
# Predict on training set
lda2.pred <- predict(lda2.fit, train)
lda2.class <- lda2.pred$class
table(lda2.class, train$case)
mean(lda2.class == train$case)

# Predict on testing set
lda2.pred_test <- predict(lda2.fit, test)
lda2.class <- lda2.pred_test$class
table(lda2.class, test$case)
mean(lda2.class == test$case)
```

The performance does not improved. This may caused from the non-Gaussian distribution of the data.

(3) Quadratic Discriminant Analysis

```{r, fig.height = 6, fig.width = 6}
qda.fit <- qda(case ~ Cl.thickness + Cell.size + Cell.shape + 
                      Marg.adhesion + Epith.c.size + Bare.nuclei + 
                      Bl.cromatin + Normal.nucleoli + Mitoses, 
               data = train)
qda.fit
```

```{r, fig.height = 6, fig.width = 6}
# Predict on training set
qda.pred <- predict(qda.fit, train)
qda.class <- qda.pred$class
table(qda.class, train$case)
mean(qda.class == train$case)

# Predict on testing set
qda.pred_test <- predict(qda.fit, test)
qda.class <- qda.pred_test$class
table(qda.class, test$case)
mean(qda.class == test$case)
```

QDA performs as LDA. No significant difference.

(4) KNN

Let's use a hieuristic KNN with 1 neighbors.

```{r, fig.height = 6, fig.width = 6}
knn.pred <- knn(train, test, train$case, k = 1)
table(knn.pred, test$case)
mean(knn.pred == test$case)
```

Now, consider the case $K=1\sim 10$. Plot the accuracy along with the value of $K$.

```{r, fig.height = 6, fig.width = 6}
accuracy = c()
K = c(1:50)
for(k in K)
{
  knn.pred <- knn(train, test, train$case, k = k)
  acc <- mean(knn.pred == test$case)
  accuracy <- c(accuracy, acc)
}
plot(K, accuracy, type = "l", ylim = c(0.9, 1))
```

Hence the case $K=1$ is the most simple model with the best accuracy 96.5%.


### 3. Report the performance of your classifiers 

Beside the discussions of the classifiers, we sum up the performance (testing accuracy) of each classifiers in the following:

(1) Logistic regression: 95%

(2) LDA: 93%

(3) QDA: 93.5%

(4) KNN: 96.5%

As a final remark, though KNN achieves the highest score; however, there is still a chance that logistic regression outperforms it. The data has high correlation, some dimension reduction methods may help.

### 4. Make your conclusions on data contents

To inspect whether one has a breast cancer, we may consider the features \texttt{Cl.thickness}, \texttt{Marg.adhesion }, \texttt{Bare.nuclei}, \texttt{Bl.cromatin} and \texttt{Normal.nucleoli}. 

## Problem 2: Glass Data

### 1. EDA

These data consist of 214 examples of the chemical analysis of 6 different types of glass (the target class to be predicted). There are 9 chemical variables for glass classification.

```{r, fig.height = 9, fig.width = 9}
data(Glass)
head(Glass)
```

```{r, fig.height = 9, fig.width = 9}
#View(Glass)
summary(Glass)
```

(1) Plot the scatter plot and the histrogram of the dataset

```{r, fig.height = 9, fig.width = 9}
pairs(Glass[,1:9], col=Glass[,10], pch="+") #view data (colored by glass type)
```

In this figure, different colors means a different class of glass. Some features are shown to be have a structure, such as \texttt{Al},\texttt{Si}, and \texttt{Na}.

```{r, fig.height = 9, fig.width = 9}
dat2 = data.frame(Glass)
hist.data.frame(dat2[,1:9])
```

Now the distribution looks more likely a bell-shaped. Also, since the correlation between features are smaller than that in problem 1 (see below), I expect the classifications would be easier than problem 1.

(2) Plot the correlation matrix of the dataset

```{r, fig.height = 6, fig.width = 6}
round(cor(dat2[,1:9]),2) #only for numeric variables
corrplot(cor(dat2[,1:9]))
```

The correlation between features are mostly weak besides \texttt{Ca} and \texttt{Rl}. I've found that some glasses add CaCO$_3$ during manufacturing. This may cause the refractive index increases if more are added.

(3) Boxplot

```{r, fig.height = 9, fig.width = 9}
boxplot(Glass)
```

This figure shows that glass are mostly made of silicon, then Na and Ca. Other elements contains a small proportion.

### 2. Performing the classification task

Do the train test split first. The splitting proportion is set to 0.7.

```{r, fig.height = 6, fig.width = 6}
set.seed(48763)
sample <- sample(c(TRUE, FALSE), nrow(dat2), replace=TRUE, prob=c(0.7,0.3))
train <- dat2[sample, ]
test <- dat2[!sample, ]
train.x <- as.matrix(train[1:9])
train.y <- as.matrix(train[10])
test.x <- as.matrix(test[1:9])
test.y <- as.matrix(test[10])
```

(1) Logistic Regression

```{r, fig.height = 6, fig.width = 6}
# fitting via glmnet
mod.glmnet <- glmnet::glmnet(
  x = train.x, 
  y = train.y,
  family = "multinomial"
)
```

```{r, fig.height = 6, fig.width = 6}
# Predicting on the training set
predicted_classes <-predict(object = mod.glmnet, 
                            newx = train.x, 
                            type = "class")
mean(predicted_classes == train$Type) # Model accuracy

# Predicting on the test set
predicted_classes <-predict(object = mod.glmnet, 
                            newx = test.x, 
                            type = "class")
mean(predicted_classes == test$Type) # Model accuracy
```

Since this is a logistic regression of multiple class, we cannot use backward selection [3]. All we can do is putting all the features in. then we found that the performance is poor with a 59% test accuracy.

(2) LDA

```{r, fig.height = 6, fig.width = 6}
lda.fit <- lda(Type ~ RI + Na + Mg + Al + Si + K + Ca + Ba + Fe,
               data = train)
lda.fit
plot(lda.fit)
```

```{r, fig.height = 6, fig.width = 6}
# Predict on training set
lda.pred <- predict(lda.fit, train)
lda.class <- lda.pred$class
table(lda.class, train$Type)
mean(lda.class == train$Type)

# Predict on testing set
lda.pred_test <- predict(lda.fit, test)
lda.class <- lda.pred_test$class
table(lda.class, test$Type)
mean(lda.class == test$Type)
```

LDA also performs badly, as expected. Besides, QDA cannot be performed because I encounter the error [4]. This seems like the problem for the dataset itself.

(3) KNN

```{r, fig.height = 6, fig.width = 6}
knn.pred <- knn(train, test, train$Type, k = 1)
table(knn.pred, test$Type)
mean(knn.pred == test$Type)
```

Tuning $K$ such that KNN has the best performance.

```{r, fig.height = 6, fig.width = 6}
accuracy = c()
K = c(1:50)
for(k in K)
{
  knn.pred <- knn(train, test, train$Type, k = k)
  acc <- mean(knn.pred == test$Type)
  accuracy <- c(accuracy, acc)
}
plot(K, accuracy, type = "l", ylim = c(0.7, 1))
```

Hence the case $K=1$ is the best model.

### 3. Report the performance of your classifiers

In this case, the logistic regression and LDA shows a low performance compared to KNN. 

(1) Logistic regression: 59.58%

(2) LDA: 60%

(3) QDA: Cannot perform

(4) KNN: 95.38%

### 4. Make your conclusions on data contents

Since KNN gives the best performance, we may consider that the same class of glass tends to share similar proportion of ingredients.

## Reference

[1] Stepwise regression, https://en.wikipedia.org/wiki/Stepwise_regression

[2] Glass, https://www.congcal.com/markets/glass/?doing_wp_cron=1666634304.4064259529113769531250

[3] When using glmnet how to report p-value significance to claim significance of predictors? https://stats.stackexchange.com/questions/45449/when-using-glmnet-how-to-report-p-value-significance-to-claim-significance-of-pr

[4] R Error : some group is too small for 'qda', https://stackoverflow.com/questions/20481772/r-error-some-group-is-too-small-for-qda
